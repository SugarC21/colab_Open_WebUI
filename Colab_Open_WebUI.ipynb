{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SugarC21/colab_Open_WebUI/blob/main/Colab_Open_WebUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YozUynhgdLmS"
      },
      "source": [
        "# **Open WebUI + Ollama in Colab**\n",
        "\n",
        "This notebook sets up **Open WebUI** (Python 3.11) and **Ollama** on Google Colab with optional Google Drive persistence.\n",
        "\n",
        "## **Key Adjustments**\n",
        "1. **Google Drive is mounted with `noexec`**. Executing binaries directly from Drive triggers a **`PermissionError`**.\n",
        "2. **We store** (persist) the **Ollama** binary in Drive **for reuse**, but **copy** it to **local disk** (`/content`) at runtime to actually execute it.\n",
        "3. **Open WebUI** environment can still live in Drive if desired. We reference `open-webui` using an **absolute path** so it won't cause `FileNotFoundError`.\n",
        "4. **ngrok** is used to expose only the Open WebUI interface (port **8081**). Ollama remains on port **11422**, hidden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_form"
      },
      "source": [
        "#@title **Setup**\n",
        "use_gdrive = True #@param {type:\"boolean\"}\n",
        "use_ngrok_auth = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "\n",
        "BASE_PATH = \"/content\"\n",
        "if use_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Open-WebUI\"\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Using base path:\", BASE_PATH)\n",
        "print(\"Using ngrok auth token:\", use_ngrok_auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btjRPuErdLmV"
      },
      "source": [
        "## (Optional) ngrok Authentication Token\n",
        "If you want a stable subdomain or advanced ngrok features:\n",
        "1. Toggle `use_ngrok_auth` above.\n",
        "2. Provide `ngrok_auth_token` via **Colab secrets** or an **environment variable**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQp3h0ktdLmV"
      },
      "source": [
        "## Install Dependencies\n",
        "- Installs **Python 3.11**, **venv**, **dev** packages.\n",
        "- Installs **pciutils** and **lshw**.\n",
        "- The system is ephemeral, so if you do **not** use Google Drive, everything resets on runtime end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.11 python3.11-venv python3.11-dev pciutils lshw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nmlSdzdLmW"
      },
      "source": [
        "## Install or Retrieve Ollama\n",
        "**Key Point**: Because Drive is mounted with `noexec`, we can't run binaries directly from Drive.\n",
        "1. If `use_gdrive` is **True**, we'll store the Ollama binary in Drive (for persistence)\n",
        "2. On each run, we **copy** it from Drive to `/content/ollama.bin`, then execute it from local ephemeral disk.\n",
        "3. If not using Drive, we install Ollama system-wide each time and run it from `/usr/local/bin/ollama` (or similar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_ollama"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "ollama_drive_dir = os.path.join(BASE_PATH, 'ollama')\n",
        "ollama_drive_bin = os.path.join(ollama_drive_dir, 'ollama')\n",
        "ollama_local_bin = \"/content/ollama.bin\"  # We'll copy from Drive or system-wide here\n",
        "\n",
        "if use_gdrive:\n",
        "    if os.path.exists(ollama_drive_bin):\n",
        "        print(\"Ollama binary found in Drive.\")\n",
        "        print(\"Copying it to local disk to bypass 'noexec'...\")\n",
        "        !cp \"$ollama_drive_bin\" \"$ollama_local_bin\"\n",
        "        !chmod +x \"$ollama_local_bin\"\n",
        "    else:\n",
        "        print(\"No Ollama binary in Drive. Installing Ollama system-wide...\")\n",
        "        !curl -fsSL https://ollama.com/install.sh | sh\n",
        "        which_ollama = subprocess.check_output(['which', 'ollama']).decode().strip()\n",
        "        print(\"Copying system-wide ollama to Drive AND local disk...\")\n",
        "        os.makedirs(ollama_drive_dir, exist_ok=True)\n",
        "        !cp \"$which_ollama\" \"$ollama_drive_bin\"\n",
        "        !chmod +x \"$ollama_drive_bin\"\n",
        "        !cp \"$which_ollama\" \"$ollama_local_bin\"\n",
        "        !chmod +x \"$ollama_local_bin\"\n",
        "        print(\"Ollama persisted in Drive and copied to local disk.\")\n",
        "else:\n",
        "    print(\"Installing Ollama system-wide (ephemeral)\")\n",
        "    !curl -fsSL https://ollama.com/install.sh | sh\n",
        "    print(\"Copying ephemeral install to local disk...\")\n",
        "    which_ollama = subprocess.check_output(['which', 'ollama']).decode().strip()\n",
        "    !cp \"$which_ollama\" \"$ollama_local_bin\"\n",
        "    !chmod +x \"$ollama_local_bin\"\n",
        "    print(\"Ollama is ready at\", ollama_local_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQCNdVqsdLmW"
      },
      "source": [
        "## Set Up Virtual Environment & Install Open WebUI\n",
        "We can store **Open WebUI** in Drive for persistence. Running Python from Drive is allowed, but we must reference it via an absolute path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "open_webui_install"
      },
      "source": [
        "venv_path = os.path.join(BASE_PATH, 'venv')\n",
        "if not os.path.exists(venv_path):\n",
        "    print(\"Creating Python 3.11 virtual environment...\")\n",
        "    !python3.11 -m venv \"$venv_path\"\n",
        "\n",
        "print(\"Upgrading pip in venv...\")\n",
        "!\"{venv_path}/bin/python\" -m pip install --upgrade pip\n",
        "\n",
        "print(\"Installing Open WebUI...\")\n",
        "!\"{venv_path}/bin/pip\" install open-webui\n",
        "\n",
        "print(\"Open WebUI installation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F2KYwAtdLmX"
      },
      "source": [
        "## Create a Script to Start Both Servers\n",
        "- **Ollama** will be run from the local ephemeral binary at `/content/ollama.bin`.\n",
        "- **Open WebUI** will be called via the **absolute path** to the `open-webui` CLI in the virtual environment.\n",
        "- We use Pythonâ€™s `subprocess` to launch each in separate threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "start_servers"
      },
      "source": [
        "import os\n",
        "\n",
        "server_script_path = os.path.join(BASE_PATH, 'start_servers.py')\n",
        "ollama_local_bin = \"/content/ollama.bin\"  # from above steps\n",
        "open_webui_bin = os.path.join(venv_path, 'bin', 'open-webui')\n",
        "\n",
        "# We'll run everything from any directory, so use absolute paths.\n",
        "script_content = f'''\\\n",
        "import subprocess, threading, time\n",
        "\n",
        "OLLAMA_CMD = \"{ollama_local_bin}\"\n",
        "OPEN_WEBUI_CMD = \"{open_webui_bin}\"\n",
        "\n",
        "def start_ollama():\n",
        "    subprocess.run([OLLAMA_CMD, 'serve'])\n",
        "\n",
        "def start_open_webui():\n",
        "    subprocess.run([OPEN_WEBUI_CMD, 'serve', '--port', '8081'])\n",
        "\n",
        "threading.Thread(target=start_ollama).start()\n",
        "time.sleep(5)\n",
        "threading.Thread(target=start_open_webui).start()\n",
        "'''\n",
        "\n",
        "with open(server_script_path, 'w') as f:\n",
        "    f.write(script_content)\n",
        "\n",
        "print(\"Created:\", server_script_path)\n",
        "print(\"- Ollama will run from /content/ollama.bin\")\n",
        "print(f\"- Open WebUI will run from {open_webui_bin}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6urM-tbdLmY"
      },
      "source": [
        "## Start Servers & Expose Open WebUI via ngrok\n",
        "The Open WebUI is on **port 8081**, Ollama on **port 11422** (hidden)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_ngrok"
      },
      "source": [
        "!pip install pyngrok --quiet\n",
        "\n",
        "import time, os\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_ngrok_token():\n",
        "    secret_token = userdata.get('ngrok_auth_token')\n",
        "    if secret_token:\n",
        "        return secret_token\n",
        "    env_token = os.environ.get('ngrok_auth_token', '')\n",
        "    if env_token:\n",
        "        return env_token\n",
        "    return ''\n",
        "\n",
        "if use_ngrok_auth:\n",
        "    token = get_ngrok_token()\n",
        "    if token:\n",
        "        ngrok.set_auth_token(token)\n",
        "        print(\"ngrok auth token set.\")\n",
        "    else:\n",
        "        print(\"No 'ngrok_auth_token' found. Proceeding without auth token.\")\n",
        "\n",
        "server_script_path = os.path.join(BASE_PATH, 'start_servers.py')\n",
        "venv_path = os.path.join(BASE_PATH, 'venv')\n",
        "python_bin = os.path.join(venv_path, 'bin', 'python')\n",
        "\n",
        "# Spawn the script in background using Python's subprocess\n",
        "p = subprocess.Popen([python_bin, server_script_path])\n",
        "\n",
        "# Wait briefly for servers to initialize\n",
        "time.sleep(20)\n",
        "\n",
        "print(\"\\nAttempting to open ngrok tunnel for Open WebUI (port 8081)...\")\n",
        "webui_tunnel = ngrok.connect(8081, \"http\")\n",
        "print(\"Open WebUI URL:\", webui_tunnel.public_url)\n",
        "\n",
        "ollama_tunnel = ngrok.connect(11422, \"http\")\n",
        "print(\"\\nSetup complete. Ollama is hidden, but accessible locally on port 11422.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}