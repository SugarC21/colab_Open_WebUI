{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SugarC21/colab_Open_WebUI/blob/main/Colab_Open_WebUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S087aq_XKYDn"
      },
      "source": [
        "# **Open WebUI + Ollama in Colab**\n",
        "\n",
        "This notebook installs and starts **Open WebUI** (requires Python 3.11) and **Ollama** in Google Colab, using **ngrok** for public tunneling.\n",
        "\n",
        "### **Key Features**\n",
        "1. **Optionally** mount Google Drive to install both **Open WebUI** and **Ollama** in a persistent location, so you don’t have to reinstall them each session.\n",
        "2. **Optional** ngrok authentication token for stable subdomains or other premium ngrok features.\n",
        "3. We only expose **Open WebUI** (port 8081) publicly; **Ollama** (port 11422) remains hidden/local.\n",
        "\n",
        "### **Important Notes**\n",
        "- **GPU Usage**: Currently, **Ollama** on Linux/Colab runs on **CPU** only.\n",
        "- **Colab Resource Constraints**: CPU-based inference can be slow for large models. Consider smaller or more efficient models.\n",
        "- **ngrok Auth**: If you need stable subdomains, set `use_ngrok_auth = True` and provide your token in `os.environ['ngrok_auth_token']`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_form"
      },
      "source": [
        "#@title **Setup**\n",
        "#@markdown 1. Toggle below to enable using (and saving) **both** Open WebUI and Ollama to Google Drive.\n",
        "#@markdown 2. Toggle below to enable an ngrok auth token (stable subdomain, etc.).\n",
        "\n",
        "use_gdrive = True #@param {type:\"boolean\"}\n",
        "use_ngrok_auth = False #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "\n",
        "BASE_PATH = \"/content\"\n",
        "if use_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Open-WebUI\"  # We'll store the environment & Ollama here\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Using base path:\", BASE_PATH)\n",
        "print(\"Using ngrok auth token:\", use_ngrok_auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KUbrlMVKYDr"
      },
      "source": [
        "### **Brief Instruction for ngrok Authentication Token**\n",
        "- If you want a **stable** or **custom subdomain** (or other features), you need an [ngrok](https://ngrok.com/) account with an auth token.\n",
        "- **Storing the Token in a Colab Secret**:\n",
        "  1. Go to **Runtime** > **RunTime manager** > **Secrets** (UI can vary).\n",
        "  2. Create a new secret named `ngrok_auth_token` with your token as the value.\n",
        "  3. Once saved, you can retrieve it with `os.environ.get('ngrok_auth_token')`.\n",
        "- Alternatively, you can do:\n",
        "  ```bash\n",
        "  %env ngrok_auth_token=YOUR_TOKEN_HERE\n",
        "  ```\n",
        "- Toggle `use_ngrok_auth = True` in the form above if you want to use the token.\n",
        "\n",
        "If no token is provided (or if `use_ngrok_auth` is **False**), you'll still get a random temporary tunnel for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yveHyPnGKYDr"
      },
      "source": [
        "## **Install Dependencies**\n",
        "1. Update apt.\n",
        "2. Install Python 3.11, venv, and dev packages.\n",
        "3. Install system tools. (pciutils, lshw, etc.)\n",
        "\n",
        "*(This step might take a few minutes.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.11 python3.11-venv python3.11-dev pciutils lshw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjUyQsA0KYDr"
      },
      "source": [
        "## **Install Ollama**\n",
        "- If **use_gdrive** is **True**, we will download the Linux x86_64 tarball into Drive and run it directly from there, so it persists.\n",
        "- Otherwise, we run the **official install script** (`curl -fsSL https://ollama.com/install.sh | sh`), installing Ollama system-wide in the ephemeral Colab VM.\n",
        "\n",
        "**Note**: On Linux, Ollama currently runs **CPU-only**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_ollama"
      },
      "source": [
        "if use_gdrive:\n",
        "    print(\"Persisting Ollama to Drive...\")\n",
        "    import os\n",
        "    ollama_dir = os.path.join(BASE_PATH, 'ollama')\n",
        "    ollama_bin = os.path.join(ollama_dir, 'ollama')\n",
        "\n",
        "    if os.path.exists(ollama_bin):\n",
        "        print(\"Ollama binary already exists in Drive.\")\n",
        "        print(\"To update, remove or overwrite the folder or manually download a newer version.\")\n",
        "    else:\n",
        "        print(\"Downloading Ollama (Linux x86_64) into Drive...\")\n",
        "        os.makedirs(ollama_dir, exist_ok=True)\n",
        "\n",
        "        # Download a pinned version or latest stable\n",
        "        OLLAMA_VERSION = \"v0.0.16\"  # Example pinned version\n",
        "        DOWNLOAD_URL = f\"https://github.com/jmorganca/ollama/releases/download/{OLLAMA_VERSION}/ollama-{OLLAMA_VERSION}-Linux-x86_64.tar.gz\"\n",
        "\n",
        "        # Download & extract\n",
        "        !wget -q \"$DOWNLOAD_URL\" -O /tmp/ollama.tar.gz\n",
        "        !tar -xzf /tmp/ollama.tar.gz -C \"$ollama_dir\" --strip-components 1\n",
        "        !chmod +x \"$ollama_bin\"\n",
        "        print(\"Ollama persisted to Drive at:\", ollama_bin)\n",
        "\n",
        "else:\n",
        "    print(\"Installing Ollama system-wide (ephemeral)\")\n",
        "    !curl -fsSL https://ollama.com/install.sh | sh\n",
        "    print(\"Ollama installed for this session.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l26p42HsKYDs"
      },
      "source": [
        "## **Set Up Virtual Environment & Install Open WebUI**\n",
        "We’ll use Python 3.11 in a virtual environment. If you enabled Google Drive, everything goes into `/content/drive/MyDrive/Open-WebUI`. Otherwise, it goes into `/content`.\n",
        "\n",
        "This includes the **Open WebUI** Python package and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "open_webui_install"
      },
      "source": [
        "import os\n",
        "\n",
        "venv_path = os.path.join(BASE_PATH, \"venv\")\n",
        "\n",
        "if not os.path.exists(venv_path):\n",
        "    print(\"Creating a new Python 3.11 virtual environment...\")\n",
        "    !python3.11 -m venv \"$venv_path\"\n",
        "\n",
        "print(\"Upgrading pip in the virtual environment...\")\n",
        "!\"$venv_path/bin/python\" -m pip install --upgrade pip\n",
        "\n",
        "print(\"Installing Open WebUI...\")\n",
        "!\"$venv_path/bin/pip\" install open-webui\n",
        "\n",
        "print(\"Open WebUI installation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A29NEpB7KYDs"
      },
      "source": [
        "## **Create a Script to Start Both Servers (Ollama & Open WebUI)**\n",
        "We will run:\n",
        "- **Ollama** on port **11422** in one thread\n",
        "- **Open WebUI** on port **8081** in another\n",
        "\n",
        "### Where does Ollama run?\n",
        "- If `use_gdrive` is **True**, we run the binary from `BASE_PATH/ollama/ollama`.\n",
        "- Otherwise, we call the system-wide `ollama` command installed via the shell script.\n",
        "\n",
        "All output is suppressed for a cleaner notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "start_servers"
      },
      "source": [
        "import os\n",
        "\n",
        "server_script_path = os.path.join(BASE_PATH, 'start_servers.py')\n",
        "if use_gdrive:\n",
        "    ollama_bin = os.path.join(BASE_PATH, 'ollama', 'ollama')\n",
        "else:\n",
        "    ollama_bin = 'ollama'  # ephemeral system-wide\n",
        "\n",
        "script_content = f'''\\\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "OLLAMA_CMD = \"{ollama_bin}\"\n",
        "\n",
        "def start_ollama():\n",
        "    subprocess.run([OLLAMA_CMD, 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "def start_open_webui():\n",
        "    subprocess.run(['./venv/bin/open-webui', 'serve', '--port', '8081'], cwd='.', stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "threading.Thread(target=start_ollama).start()\n",
        "time.sleep(5)  # Wait a bit to ensure Ollama is up\n",
        "threading.Thread(target=start_open_webui).start()\n",
        "'''\n",
        "\n",
        "with open(server_script_path, 'w') as f:\n",
        "    f.write(script_content)\n",
        "\n",
        "print(f\"Created script at: {server_script_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0DWdgvGKYDt"
      },
      "source": [
        "## **Start Servers & Expose the Open WebUI Port via ngrok**\n",
        "1. **Run `start_servers.py`** in the background.\n",
        "2. Wait ~20 seconds to ensure the processes have started.\n",
        "3. **Connect** to port **8081** (Open WebUI) using **ngrok**.\n",
        "4. Ollama runs locally on port **11422** and is **not** shown (no public URL).\n",
        "\n",
        "### **Output**\n",
        "- Only the **Open WebUI** tunnel link is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_ngrok"
      },
      "source": [
        "!pip install pyngrok --quiet\n",
        "\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# If user wants to use an auth token, set it from the environment.\n",
        "if use_ngrok_auth:\n",
        "    token = os.environ.get('ngrok_auth_token', '')\n",
        "    if token:\n",
        "        ngrok.set_auth_token(token)\n",
        "        print(\"ngrok authentication token set!\")\n",
        "    else:\n",
        "        print(\"No 'ngrok_auth_token' found in environment. Proceeding without an auth token.\")\n",
        "\n",
        "print(\"Starting servers...\")\n",
        "!\"$venv_path/bin/python\" \"$server_script_path\" &\n",
        "time.sleep(20)  # give them time to start\n",
        "\n",
        "print(\"\\nConnecting to Open WebUI on port 8081 via ngrok...\")\n",
        "webui_tunnel = ngrok.connect(8081, \"http\")\n",
        "print(\"Open WebUI URL:\", webui_tunnel.public_url)\n",
        "\n",
        "# Create the Ollama tunnel but do NOT print it.\n",
        "ollama_tunnel = ngrok.connect(11422, \"http\")\n",
        "\n",
        "print(\"\\nAll set! Use the Open WebUI URL above to access your interface.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}